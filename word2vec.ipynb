{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create necessary directories:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"./data/\").mkdir(exist_ok=True)\n",
    "Path(\"./models/\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, is_skip_gram: bool = True):\n",
    "        \"\"\"\n",
    "        Word2Vec model implementation in PyTorch\n",
    "\n",
    "        Args:\n",
    "            vocab_size: Size of vocabulary\n",
    "            embedding_dim: Dimension of word embeddings\n",
    "            is_skip_gram: If True, use Skip-gram model. If False, use CBOW\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.is_skip_gram = is_skip_gram\n",
    "\n",
    "        # Input embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize embeddings and linear layer weights\"\"\"\n",
    "        initrange = 0.5 / self.embedding_dim\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output.weight.data.uniform_(-initrange, initrange)\n",
    "        self.output.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of word indices\n",
    "               For Skip-gram: shape (batch_size, 1)\n",
    "               For CBOW: shape (batch_size, context_size)\n",
    "\n",
    "        Returns:\n",
    "            Output logits of shape (batch_size, vocab_size)\n",
    "        \"\"\"\n",
    "        if self.is_skip_gram:\n",
    "            embeds = self.embedding(x).squeeze(1)\n",
    "        else:\n",
    "            embeds = self.embedding(x).mean(dim=1)\n",
    "\n",
    "        return self.output(embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, texts: List[str], window_size: int, min_count: int, is_skip_gram: bool\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Dataset for training Word2Vec model\n",
    "\n",
    "        Args:\n",
    "            texts: List of tokenized texts\n",
    "            window_size: Size of context window\n",
    "            min_count: Minimum frequency for words to be included\n",
    "            is_skip_gram: If True, generate Skip-gram pairs. If False, generate CBOW pairs\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.is_skip_gram = is_skip_gram\n",
    "\n",
    "        # Build vocabulary\n",
    "        word_counts = Counter([word for text in texts for word in text])\n",
    "        self.vocab = {\n",
    "            word: idx + 1  # Reserve 0 for padding\n",
    "            for idx, (word, count) in enumerate(word_counts.items())\n",
    "            if count >= min_count\n",
    "        }\n",
    "        self.vocab[\"<pad>\"] = 0\n",
    "        self.inverse_vocab = {idx: word for word, idx in self.vocab.items()}\n",
    "\n",
    "        # Generate training pairs\n",
    "        self.pairs = self._generate_pairs(texts)\n",
    "\n",
    "    def _generate_pairs(\n",
    "        self, texts: List[str]\n",
    "    ) -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"Generate input-target pairs for training\"\"\"\n",
    "        pairs = []\n",
    "\n",
    "        for text in texts:\n",
    "            word_indices = [self.vocab.get(word, 0) for word in text]\n",
    "\n",
    "            for i in range(len(text)):\n",
    "                # Generate context window\n",
    "                context_start = max(0, i - self.window_size)\n",
    "                context_end = min(len(text), i + self.window_size + 1)\n",
    "                context = (\n",
    "                    word_indices[context_start:i] + word_indices[i + 1 : context_end]\n",
    "                )\n",
    "\n",
    "                if len(context) == 0:\n",
    "                    continue\n",
    "\n",
    "                if self.is_skip_gram:\n",
    "                    # Skip-gram: predict context words from center word\n",
    "                    center = word_indices[i]\n",
    "                    for ctx in context:\n",
    "                        if ctx != 0:  # Skip padding\n",
    "                            pairs.append((torch.tensor([center]), torch.tensor(ctx)))\n",
    "                else:\n",
    "                    # CBOW: predict center word from context words\n",
    "                    if word_indices[i] == 0:  # Skip padding\n",
    "                        continue\n",
    "\n",
    "                    # Pad context to fixed size\n",
    "                    ctx_size = 2 * self.window_size\n",
    "                    ctx_padded = context + [0] * (ctx_size - len(context))\n",
    "                    ctx_padded = ctx_padded[:ctx_size]\n",
    "\n",
    "                    pairs.append(\n",
    "                        (torch.tensor(ctx_padded), torch.tensor(word_indices[i]))\n",
    "                    )\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.pairs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int = 100,\n",
    "        window_size: int = 5,\n",
    "        min_count: int = 5,\n",
    "        batch_size: int = 32,\n",
    "        epochs: int = 5,\n",
    "        learning_rate: float = 0.001,\n",
    "        is_skip_gram: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Word2Vec trainer\n",
    "\n",
    "        Args:\n",
    "            embedding_dim: Dimension of word embeddings\n",
    "            window_size: Size of context window\n",
    "            min_count: Minimum frequency of words to consider\n",
    "            batch_size: Training batch size\n",
    "            epochs: Number of training epochs\n",
    "            learning_rate: Learning rate for optimizer\n",
    "            is_skip_gram: If True, use Skip-gram model. If False, use CBOW\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.window_size = window_size\n",
    "        self.min_count = min_count\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_skip_gram = is_skip_gram\n",
    "\n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            format=\"%(asctime)s : %(levelname)s : %(message)s\", level=logging.INFO\n",
    "        )\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        texts: List[List[str]],\n",
    "        output_path: Optional[str] = None,\n",
    "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ) -> Tuple[Word2VecModel, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Train Word2Vec model\n",
    "\n",
    "        Args:\n",
    "            texts: List of tokenized texts\n",
    "            output_path: Optional path to save trained model\n",
    "            device: Device to train on ('cuda' or 'cpu')\n",
    "\n",
    "        Returns:\n",
    "            Trained model and word embeddings dictionary\n",
    "        \"\"\"\n",
    "        # Create dataset\n",
    "        dataset = Word2VecDataset(\n",
    "            texts, self.window_size, self.min_count, self.is_skip_gram\n",
    "        )\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Initialize model\n",
    "        model = Word2VecModel(\n",
    "            len(dataset.vocab), self.embedding_dim, self.is_skip_gram\n",
    "        ).to(device)\n",
    "\n",
    "        # Initialize optimizer and loss\n",
    "        optimizer = optim.Adam(model.parameters(), lr=self.learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Training loop\n",
    "        print(f\"Training Word2Vec model on {len(texts)} texts...\")\n",
    "        model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for batch_idx, (x, y) in enumerate(dataloader):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x)\n",
    "                loss = criterion(output, y)\n",
    "\n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if (batch_idx + 1) % 100 == 0:\n",
    "                    print(\n",
    "                        f\"Epoch {epoch + 1}/{self.epochs}, \"\n",
    "                        f\"Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
    "                        f\"Loss: {total_loss / (batch_idx + 1):.4f}\"\n",
    "                    )\n",
    "\n",
    "            print(\n",
    "                f\"Epoch {epoch + 1} completed, \"\n",
    "                f\"Average Loss: {total_loss / len(dataloader):.4f}\"\n",
    "            )\n",
    "\n",
    "        # Save model if output path provided\n",
    "        if output_path:\n",
    "            output_path = Path(output_path)\n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"vocab\": dataset.vocab,\n",
    "                    \"embedding_dim\": self.embedding_dim,\n",
    "                    \"is_skip_gram\": self.is_skip_gram,\n",
    "                },\n",
    "                output_path,\n",
    "            )\n",
    "            print(f\"Model saved to {output_path}\")\n",
    "\n",
    "        # Create word embeddings dictionary\n",
    "        embeddings = {\n",
    "            word: model.embedding.weight.data[idx].cpu()\n",
    "            for word, idx in dataset.vocab.items()\n",
    "        }\n",
    "\n",
    "        return model, embeddings\n",
    "\n",
    "    def load_model(\n",
    "        self, path: str, device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    ) -> Tuple[Word2VecModel, Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Load saved model\n",
    "\n",
    "        Args:\n",
    "            path: Path to saved model\n",
    "            device: Device to load model on\n",
    "\n",
    "        Returns:\n",
    "            Loaded model and vocabulary\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "\n",
    "        model = Word2VecModel(\n",
    "            len(checkpoint[\"vocab\"]),\n",
    "            checkpoint[\"embedding_dim\"],\n",
    "            checkpoint[\"is_skip_gram\"],\n",
    "        ).to(device)\n",
    "\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        return model, checkpoint[\"vocab\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(word: str, embeddings, n: int = 5) -> List[Tuple[str, float]]:\n",
    "    if word not in embeddings:\n",
    "        return []\n",
    "\n",
    "    word_embedding = embeddings[word]\n",
    "    similarities = []\n",
    "\n",
    "    for w, embed in embeddings.items():\n",
    "        if w != word:\n",
    "            cos_sim = nn.functional.cosine_similarity(\n",
    "                word_embedding.unsqueeze(0), embed.unsqueeze(0)\n",
    "            )\n",
    "            similarities.append((w, cos_sim.item()))\n",
    "\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"sdwalker62/TinyStoriesWithValidationSet\")\n",
    "ds = ds.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ds` is a `DatasetDict` object that contains three splits; one for training, one for tuning, and the other for our test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2098521\n",
       "    })\n",
       "    validate: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21198\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Timmy was talking to his mom and dad. He said, \"Can I play?\" His mom said, \"No Timmy, it\\'s time to do your work.\" Timmy said, \"But I don\\'t want to do it!\" His dad said, \"It\\'s ok, Timmy. You can do it. I recommend that you work a little bit.\" Timmy said, \"Ok, dad.\" \\n\\nSo, Timmy got his work ready and he worked hard. After a few minutes, he was done and it looked perfect. His dad said, \"Very good work, Timmy. You did a great job!\" Timmy said, \"Thanks, dad!\" He was very proud of his work and smiled. \\n\\nThe end.'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model on 2 texts...\n",
      "Epoch 1 completed, Average Loss: 2.8333\n",
      "Epoch 2 completed, Average Loss: 2.8324\n",
      "Epoch 3 completed, Average Loss: 2.8314\n",
      "Epoch 4 completed, Average Loss: 2.8303\n",
      "Epoch 5 completed, Average Loss: 2.8290\n",
      "Model saved to models/word2vec.pt\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"],\n",
    "    [\"machine\", \"learning\", \"is\", \"a\", \"subset\", \"of\", \"artificial\", \"intelligence\"],\n",
    "]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Word2VecTrainer(\n",
    "    embedding_dim=100,\n",
    "    window_size=2,\n",
    "    min_count=1,  # Set to 1 for this small example\n",
    "    epochs=5,\n",
    "    is_skip_gram=True,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model, embeddings = trainer.train(texts, output_path=\"models/word2vec.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words most similar to 'fox':\n",
      "the: 0.6072\n",
      "brown: 0.4532\n",
      "jumps: 0.3941\n",
      "lazy: 0.2456\n",
      "quick: 0.1852\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    similar_words = find_similar(\"fox\", embeddings, n=5)\n",
    "    print(\"\\nWords most similar to 'fox':\")\n",
    "    for word, score in similar_words:\n",
    "        print(f\"{word}: {score:.4f}\")\n",
    "except KeyError:\n",
    "    print(\"Word 'fox' not in vocabulary or insufficient training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w2v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
